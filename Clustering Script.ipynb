{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1cf4a-3ef7-4bc5-aa22-053c3e24cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "##################################\n",
    "## Step 1: Read in, format data ##\n",
    "##################################\n",
    "\n",
    "# input name of file with data\n",
    "file_path = input(\"Enter name of survey output file: \")\n",
    "\n",
    "# read in data\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data successfully read\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"No data found in file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# name of relevant columns to extract from file\n",
    "names_col = \"First and Last Name\"\n",
    "email_col = \"BU email\"\n",
    "alreadyvantrained_col = \"Have you been previously van trained by the Community Service Center?\"\n",
    "wanttovantrain_col = \"Would you like to be van trained?\"\n",
    "option1_col = \"Please check your FIRST CHOICE of run assignment.\"\n",
    "option2_col = \"Please check your SECOND CHOICE of run assignment.\"\n",
    "option3_col = \"Please check your THIRD CHOICE of run assignment.\"\n",
    "requests_col = \"Please list the names of fellow volunteers that you'd like to be placed on a run with. Please note that we cannot guarantee that you will receive the same run assignment, especially if you do not have overlapping availability. \\nLeave blank if N/A.\\nEnter just first and last names separated by commas: Eleanor Hoffpauir, Ocean Bruinius\"\n",
    "\n",
    "# extract relevant columns from file\n",
    "columns_to_extract = [names_col,email_col,alreadyvantrained_col,wanttovantrain_col,option1_col,option2_col,option3_col,requests_col]\n",
    "data = data[columns_to_extract]\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "data[alreadyvantrained_col] = data[alreadyvantrained_col].replace({'Yes': 1, 'No': 0})\n",
    "data[wanttovantrain_col] = data[wanttovantrain_col].replace({'Yes': 1, 'No': 0})\n",
    "data['vantrained'] = (data[alreadyvantrained_col] == 1) | (data[wanttovantrain_col] == 1)\n",
    "data['vantrained'] = data['vantrained'].astype(int)\n",
    "\n",
    "data[names_col] = data[names_col].str.rstrip()\n",
    "\n",
    "# gather timeslots\n",
    "option_cols = [option1_col,option2_col,option3_col]\n",
    "unique_values = set()\n",
    "for column in option_cols:\n",
    "    unique_values.update(data[column].unique())\n",
    "\n",
    "num_timeslots = len(unique_values)\n",
    "\n",
    "# add encodings to dataframe\n",
    "for value in unique_values:\n",
    "    data[value]=0\n",
    "\n",
    "# transform to many-hot encodings\n",
    "for col in option_cols:\n",
    "    for index, row in data.iterrows():\n",
    "        value = row[col]\n",
    "        # Set the corresponding column that matches 'value' to 1\n",
    "        data.at[index, value] = 1\n",
    "\n",
    "# drop original columns\n",
    "data = data.drop(option_cols,axis=1)\n",
    "\n",
    "data = data.drop_duplicates(subset=email_col,keep='last')\n",
    "data = data.drop_duplicates(subset=names_col,keep='last')\n",
    "\n",
    "data = pd.concat([data, data[requests_col].str.split(', ', expand=True)], axis=1)\n",
    "data = data.drop(columns=[requests_col])\n",
    "\n",
    "###############################\n",
    "## Step 2: Fuzzy match names ##\n",
    "###############################\n",
    "\n",
    "ground_truth = data[names_col]\n",
    "fuzzy =  set()\n",
    "\n",
    "for column_name in data.columns:\n",
    "    # Check if the column name is an integer\n",
    "    if isinstance(column_name, int):\n",
    "        # Update the unique values set with the unique values from this column\n",
    "        fuzzy.update(data[column_name].unique())\n",
    "\n",
    "ground_truth = ground_truth.apply(str.strip)\n",
    "fuzzy = {str(name).strip() for name in fuzzy}\n",
    "\n",
    "# Initialize an empty dictionary to store matches\n",
    "matches = {}\n",
    "\n",
    "# Iterate over each name in the fuzzy set\n",
    "for name in fuzzy:\n",
    "    # Use process.extractOne to find the best match with the ratio scorer\n",
    "    best_match = process.extractOne(name, ground_truth, scorer=fuzz.ratio)\n",
    "\n",
    "    # Check if a match was found\n",
    "    if best_match:\n",
    "        # Calculate likelihood (as a probability)\n",
    "        likelihood = best_match[1] / 100.0\n",
    "        # cull matches under threshold .75\n",
    "        if likelihood >= .75:\n",
    "            # Store the result in the matches dictionary\n",
    "            matches[name] = (best_match[0])\n",
    "    else:\n",
    "        # Optionally handle cases where no match is found\n",
    "        matches[name] = (None)\n",
    "\n",
    "# Convert the result to a DataFrame for easier viewing\n",
    "result_df = pd.DataFrame.from_dict(matches, orient='index', columns=['Matched Name'])\n",
    "\n",
    "# determine which columns hold request names\n",
    "integer_columns = [col for col in data.columns if isinstance(col, int)]\n",
    "\n",
    "# Create a new DataFrame with these integer-named columns\n",
    "df_replace = data[integer_columns].copy()\n",
    "\n",
    "def replace_or_remove(value, matches):\n",
    "    if value in matches:\n",
    "        return matches[value]\n",
    "    else:\n",
    "        return np.nan  # or return a specific value if you want\n",
    "\n",
    "# Apply the function to each element in the DataFrame\n",
    "df_replace = df_replace.map(lambda x: replace_or_remove(x, matches))\n",
    "\n",
    "data = data.drop(columns=integer_columns)\n",
    "data = pd.concat([data,df_replace],axis=1)\n",
    "\n",
    "########################\n",
    "## Step 3: Clustering ##\n",
    "########################\n",
    "\n",
    "# Parameters\n",
    "n_groups = num_timeslots\n",
    "\n",
    "min_group_size = input(\"Minimum group size:\")\n",
    "seed = random.randint(0,1000000)\n",
    "#min_group_size = 3\n",
    "\n",
    "# shuffle data\n",
    "data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "# All time slots with names of columns representing them\n",
    "timeslot_columns = unique_values\n",
    "\n",
    "# save timeslot columns as iterable\n",
    "timeslot_columns = list(unique_values)\n",
    "\n",
    "network_data = []\n",
    "\n",
    "for colidx in range(0,num_timeslots):\n",
    "    timeslot_data = data[data[timeslot_columns[colidx]]==1][[names_col] +  integer_columns]\n",
    "    #print(timeslot_data)\n",
    "\n",
    "    # create node list\n",
    "    node_list = timeslot_data[names_col]\n",
    "    node_list = node_list.reset_index(drop=True)\n",
    "    #print(node_list)\n",
    "\n",
    "    # create edge list\n",
    "    edge_list = pd.melt(timeslot_data, id_vars=[names_col])\n",
    "    edge_list = edge_list.drop(columns='variable')\n",
    "    edge_list = edge_list.dropna()\n",
    "    edge_list = [tuple(x) for x in edge_list.values]\n",
    "    #print(edge_list)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    # Add nodes to the graph\n",
    "    G.add_nodes_from(node_list)\n",
    "    # Add edges to the graph\n",
    "    G.add_edges_from(edge_list)\n",
    "\n",
    "    network_data.append((timeslot_columns[colidx],node_list, edge_list, G))\n",
    "\n",
    "node_tracker = pd.DataFrame({\n",
    "    'Names': data[names_col],\n",
    "    'Occurrences': 0  # Initialize all values in the 'occurrences' column to 0\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def update_node_tracker(network_data, node_tracker):\n",
    "    # Reset all occurrences to zero before updating\n",
    "    node_tracker['Occurrences'] = 0\n",
    "\n",
    "    # Create a dictionary to keep track of occurrences\n",
    "    occurrence_dict = defaultdict(int)\n",
    "\n",
    "    # Iterate over the network_data\n",
    "    for _, node_list, _, _ in network_data:\n",
    "        # Iterate over each name in the node list\n",
    "        for name in node_list:\n",
    "            occurrence_dict[name] += 1\n",
    "\n",
    "    # Update the node_tracker with the occurrences count\n",
    "    for idx, name in enumerate(node_tracker['Names']):\n",
    "        node_tracker.loc[idx, 'Occurrences'] = occurrence_dict[name]\n",
    "\n",
    "    return node_tracker\n",
    "\n",
    "def is_node_in_edge(edge, node):\n",
    "    return node in (edge[0], edge[1])\n",
    "\n",
    "\n",
    "def process_network_data(network_data, node_tracker):\n",
    "    # find subset of node_tracker where names are used more than once\n",
    "    # print(\"cp0\")\n",
    "    subset_node_tracker = node_tracker[node_tracker['Occurrences'] > 1]\n",
    "\n",
    "    # shuffle order of nodes to randomize groupings\n",
    "    seed = random.randint(0, 100000)\n",
    "    subset_node_tracker = subset_node_tracker.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # print(\"cp1\")\n",
    "\n",
    "    for name in subset_node_tracker['Names']:\n",
    "        min_connections = float('inf')\n",
    "        chosen_index = None\n",
    "        # print(\"cp3\")\n",
    "\n",
    "        # Find all graphs that contain the name\n",
    "        for idx, (_, node_list, edge_list, graph) in enumerate(network_data):\n",
    "            if name in node_list.values:\n",
    "                num_connections = graph.degree(name)\n",
    "                # print(\"cp2\")\n",
    "\n",
    "                # If we can remove the name, check conditions\n",
    "                if num_connections < min_connections or (\n",
    "                        num_connections == min_connections and len(node_list) > len(network_data[chosen_index][1])):\n",
    "                    # Ensure at least 3 names remain after removal\n",
    "                    if len(node_list) > 3:\n",
    "                        min_connections = num_connections\n",
    "                        chosen_index = idx\n",
    "\n",
    "        # If a suitable graph is found, remove the name\n",
    "        if chosen_index is not None:\n",
    "            _, node_list, edge_list, graph = network_data[chosen_index]\n",
    "\n",
    "            # Remove the node from the graph\n",
    "            graph.remove_node(name)\n",
    "\n",
    "            # Update the node_list\n",
    "            node_list = node_list[node_list != name].reset_index(drop=True)\n",
    "\n",
    "            # Update the edge_list by filtering out edges connected to the name\n",
    "            edge_list = [edge for edge in edge_list if not is_node_in_edge(edge, name)]\n",
    "\n",
    "            # print(f\"removing {name} from {chosen_index}\")\n",
    "\n",
    "            # Update network_data\n",
    "            network_data[chosen_index] = (network_data[chosen_index][0], node_list, edge_list, graph)\n",
    "\n",
    "    return network_data\n",
    "\n",
    "def filter_edges(node_series, edge_list):\n",
    "    # Convert node_series to a set for efficient lookup\n",
    "    node_set = set(node_series.values)\n",
    "\n",
    "    # Filter edge_list to include only those edges where both nodes are in node_set\n",
    "    filtered_edges = [(node1, node2) for node1, node2 in edge_list if node1 in node_set and node2 in node_set]\n",
    "\n",
    "    return filtered_edges\n",
    "\n",
    "\n",
    "def allocate(network_data, node_tracker):\n",
    "    # initialize node tracker with counts\n",
    "    node_tracker = update_node_tracker(network_data, node_tracker)\n",
    "\n",
    "    c = 0\n",
    "\n",
    "    #print(c)\n",
    "\n",
    "    while max(node_tracker['Occurrences']) > 1:\n",
    "        network_data = process_network_data(network_data, node_tracker)\n",
    "        node_tracker = update_node_tracker(network_data, node_tracker)\n",
    "        c = c + 1\n",
    "        #print(c)\n",
    "\n",
    "allocate(network_data,node_tracker)\n",
    "\n",
    "final_data = []\n",
    "\n",
    "for i in range(len(network_data)):\n",
    "    timeslot = network_data[i][0]\n",
    "    names = network_data[i][1]\n",
    "    edges = filter_edges(network_data[i][1], network_data[i][2])\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(names)\n",
    "    G.add_edges_from(edges)\n",
    "    final_data.append((timeslot, names, edges, G))\n",
    "\n",
    "def print_groups(final_data, filename='Groups.txt'):\n",
    "    # Open or create a text file to write the output\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(len(final_data)):\n",
    "            # Write the timeslot to the file\n",
    "            f.write(f\"Timeslot: {final_data[i][0]}\\n\")\n",
    "            # Write each value, possibly with an asterisk, to the file\n",
    "            for value in final_data[i][1].values:\n",
    "                vt = \"\"\n",
    "                if data.loc[data[names_col] == value, 'vantrained'].iloc[0] == 1:\n",
    "                    vt = \"*\"\n",
    "                f.write(value + vt + \"\\n\")\n",
    "            # Write a newline between different groups\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# Create a dictionary mapping each node to its alpha based on the 'vantrained' column\n",
    "alpha_dict = data.set_index(names_col)['vantrained'].map({1: .9, 0: 0.25}).to_dict()\n",
    "\n",
    "\n",
    "def plot_all_networks(final_data):\n",
    "    pos = {}\n",
    "    plt.figure(figsize=(16, 16))\n",
    "\n",
    "    # Define a colormap for distinct colors\n",
    "    cmap = plt.colormaps['tab20']\n",
    "    colors = [cmap(i) for i in range(20)]\n",
    "\n",
    "    x_offset = 4  # Offset for discrete group separation\n",
    "    y_offset = 4\n",
    "    graph_pos = 0\n",
    "\n",
    "    # Iterate over each graph in final_data\n",
    "    for idx, data in enumerate(final_data):\n",
    "        G = data[3]  # Extract the graph from each element\n",
    "        nodes = data[1]  # Nodes are in the second item of each list element\n",
    "\n",
    "        # Create a circular layout for nodes\n",
    "        subG = G.subgraph(nodes)\n",
    "        sub_pos = nx.circular_layout(subG)\n",
    "\n",
    "        # Apply position offsets to separate individual graphs\n",
    "        for node in sub_pos:\n",
    "            sub_pos[node][0] += (graph_pos % 3) * x_offset\n",
    "            sub_pos[node][1] += (graph_pos // 3) * y_offset\n",
    "\n",
    "        # Update global positions\n",
    "        pos.update(sub_pos)\n",
    "        # Calculate the center position for the current network\n",
    "        x_vals = [pos[node][0] for node in sub_pos]\n",
    "        y_vals = [pos[node][1] for node in sub_pos]\n",
    "        center_x = sum(x_vals) / len(x_vals)\n",
    "        center_y = (sum(y_vals) / len(y_vals)) - 1.75\n",
    "\n",
    "        graph_pos += 1\n",
    "\n",
    "        # Draw nodes with a unique color for the current graph\n",
    "        nx.draw_networkx_nodes(\n",
    "            G,\n",
    "            pos,\n",
    "            nodelist=subG.nodes(),\n",
    "            node_color=[colors[idx % len(colors)]],\n",
    "            node_size=750,\n",
    "            alpha=[alpha_dict.get(node, 0.7) for node in G.nodes()],\n",
    "            label=data[0]\n",
    "        )\n",
    "\n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=G.edges(nodes), alpha=0.3, edge_color='green')\n",
    "\n",
    "        # Add a centered label for each network\n",
    "        plt.text(center_x, center_y, data[0], fontsize=7, ha='center', va='center',\n",
    "                 bbox=dict(facecolor='white', alpha=0.5, edgecolor='gray'))\n",
    "\n",
    "        # Draw labels for each node\n",
    "    labels = {node: node for node in pos}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=9)\n",
    "\n",
    "    for i in range(len(network_data)):\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=network_data[i][2], alpha=.04, edge_color='red')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"Groups.png\")\n",
    "    plt.show()\n",
    "\n",
    "##################\n",
    "## Step 4: Run! ##\n",
    "##################\n",
    "\n",
    "plot_all_networks(final_data)\n",
    "print_groups(final_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
